{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Iterative Dichotomiser 3 (ID3) Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Index is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 88\u001b[0m\n\u001b[0;32m     85\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlayTennis\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Build the tree\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m tree \u001b[38;5;241m=\u001b[39m \u001b[43mid3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecision Tree:\u001b[39m\u001b[38;5;124m\"\u001b[39m, tree)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Predict a sample\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 36\u001b[0m, in \u001b[0;36mid3\u001b[1;34m(X, y, attributes, depth, max_depth)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# If there are no more attributes or max depth is reached, return the most common label\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m attributes \u001b[38;5;129;01mor\u001b[39;00m (max_depth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m depth \u001b[38;5;241m==\u001b[39m max_depth):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Counter(y)\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Find the best attribute to split on\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Documents\\GitHub\\GitHub-Env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3190\u001b[0m, in \u001b[0;36mIndex.__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3188\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   3189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__nonzero__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m-> 3190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3191\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe truth value of a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is ambiguous. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse a.empty, a.bool(), a.item(), a.any() or a.all().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3193\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a Index is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from math import log2\n",
    "\n",
    "# Function to calculate entropy\n",
    "def entropy(y):\n",
    "    counts = Counter(y)\n",
    "    probabilities = [count / len(y) for count in counts.values()]\n",
    "    return -sum(p * log2(p) for p in probabilities)\n",
    "\n",
    "# Function to calculate the information gain\n",
    "def information_gain(X, y, attribute):\n",
    "    # Calculate the entropy of the full dataset\n",
    "    entropy_full = entropy(y)\n",
    "    \n",
    "    # Get the values and counts of the attribute\n",
    "    values, counts = np.unique(X[attribute], return_counts=True)\n",
    "    \n",
    "    # Calculate the weighted entropy for the subsets\n",
    "    entropy_subset = 0\n",
    "    for value, count in zip(values, counts):\n",
    "        subset_y = y[X[attribute] == value]\n",
    "        entropy_subset += (count / len(y)) * entropy(subset_y)\n",
    "    \n",
    "    # Calculate the information gain\n",
    "    return entropy_full - entropy_subset\n",
    "\n",
    "# Function to create the decision tree\n",
    "def id3(X, y, attributes, depth=0, max_depth=None):\n",
    "    # If all labels are the same, return the label\n",
    "    if len(set(y)) == 1:\n",
    "        return y[0]\n",
    "    \n",
    "    # If there are no more attributes or max depth is reached, return the most common label\n",
    "    if not attributes or (max_depth is not None and depth == max_depth):\n",
    "        return Counter(y).most_common(1)[0][0]\n",
    "    \n",
    "    # Find the best attribute to split on\n",
    "    gains = {attribute: information_gain(X, y, attribute) for attribute in attributes}\n",
    "    best_attribute = max(gains, key=gains.get)\n",
    "    \n",
    "    # Create the tree/sub-tree\n",
    "    tree = {best_attribute: {}}\n",
    "    \n",
    "    # Get the values of the best attribute\n",
    "    values = np.unique(X[best_attribute])\n",
    "    \n",
    "    # Recur for each subset\n",
    "    for value in values:\n",
    "        subset_X = X[X[best_attribute] == value]\n",
    "        subset_y = y[X[best_attribute] == value]\n",
    "        subtree = id3(subset_X, subset_y, [attr for attr in attributes if attr != best_attribute], depth + 1, max_depth)\n",
    "        tree[best_attribute][value] = subtree\n",
    "    \n",
    "    return tree\n",
    "\n",
    "# Function to make predictions with the decision tree\n",
    "def predict(tree, sample):\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "    attribute = next(iter(tree))\n",
    "    value = sample[attribute]\n",
    "    subtree = tree[attribute].get(value, None)\n",
    "    if subtree is None:\n",
    "        return None\n",
    "    return predict(subtree, sample)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example dataset\n",
    "    data = {\n",
    "        'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],\n",
    "        'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],\n",
    "        'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],\n",
    "        'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],\n",
    "        'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\n",
    "    }\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Features and labels\n",
    "    X = df.drop(columns='PlayTennis')\n",
    "    y = df['PlayTennis']\n",
    "\n",
    "    # Build the tree\n",
    "    tree = id3(X, y, X.columns, max_depth=3)\n",
    "    print(\"Decision Tree:\", tree)\n",
    "\n",
    "    # Predict a sample\n",
    "    sample = {'Outlook': 'Sunny', 'Temperature': 'Cool', 'Humidity': 'High', 'Wind': 'Strong'}\n",
    "    prediction = predict(tree, sample)\n",
    "    print(\"Prediction for sample:\", prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C5. Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from math import log2\n",
    "\n",
    "# Function to calculate entropy\n",
    "def entropy(y):\n",
    "    counts = Counter(y)\n",
    "    probabilities = [count / len(y) for count in counts.values()]\n",
    "    return -sum(p * log2(p) for p in probabilities)\n",
    "\n",
    "# Function to calculate the information gain\n",
    "def information_gain(X, y, attribute):\n",
    "    # Calculate the entropy of the full dataset\n",
    "    entropy_full = entropy(y)\n",
    "    \n",
    "    # Get the values and counts of the attribute\n",
    "    values, counts = np.unique(X[attribute], return_counts=True)\n",
    "    \n",
    "    # Calculate the weighted entropy for the subsets\n",
    "    entropy_subset = 0\n",
    "    for value, count in zip(values, counts):\n",
    "        subset_y = y[X[attribute] == value]\n",
    "        entropy_subset += (count / len(y)) * entropy(subset_y)\n",
    "    \n",
    "    # Calculate the information gain\n",
    "    return entropy_full - entropy_subset\n",
    "\n",
    "# Function to calculate the intrinsic value of an attribute\n",
    "def intrinsic_value(X, attribute):\n",
    "    values, counts = np.unique(X[attribute], return_counts=True)\n",
    "    total = sum(counts)\n",
    "    return -sum((count / total) * log2(count / total) for count in counts)\n",
    "\n",
    "# Function to calculate the gain ratio\n",
    "def gain_ratio(X, y, attribute):\n",
    "    gain = information_gain(X, y, attribute)\n",
    "    intrinsic = intrinsic_value(X, attribute)\n",
    "    return gain / intrinsic if intrinsic != 0 else 0\n",
    "\n",
    "# Function to create the decision tree\n",
    "def c50(X, y, attributes, depth=0, max_depth=None):\n",
    "    # If all labels are the same, return the label\n",
    "    if len(set(y)) == 1:\n",
    "        return y[0]\n",
    "    \n",
    "    # If there are no more attributes or max depth is reached, return the most common label\n",
    "    if not attributes or (max_depth is not None and depth == max_depth):\n",
    "        return Counter(y).most_common(1)[0][0]\n",
    "    \n",
    "    # Find the best attribute to split on\n",
    "    gain_ratios = {attribute: gain_ratio(X, y, attribute) for attribute in attributes}\n",
    "    best_attribute = max(gain_ratios, key=gain_ratios.get)\n",
    "    \n",
    "    # Create the tree/sub-tree\n",
    "    tree = {best_attribute: {}}\n",
    "    \n",
    "    # Get the values of the best attribute\n",
    "    values = np.unique(X[best_attribute])\n",
    "    \n",
    "    # Recur for each subset\n",
    "    for value in values:\n",
    "        subset_X = X[X[best_attribute] == value]\n",
    "        subset_y = y[X[best_attribute] == value]\n",
    "        subtree = c50(subset_X, subset_y, [attr for attr in attributes if attr != best_attribute], depth + 1, max_depth)\n",
    "        tree[best_attribute][value] = subtree\n",
    "    \n",
    "    return tree\n",
    "\n",
    "# Function to make predictions with the decision tree\n",
    "def predict(tree, sample):\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "    attribute = next(iter(tree))\n",
    "    value = sample[attribute]\n",
    "    subtree = tree[attribute].get(value, None)\n",
    "    if subtree is None:\n",
    "        return None\n",
    "    return predict(subtree, sample)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example dataset\n",
    "    data = {\n",
    "        'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],\n",
    "        'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],\n",
    "        'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],\n",
    "        'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],\n",
    "        'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\n",
    "    }\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Features and labels\n",
    "    X = df.drop(columns='PlayTennis')\n",
    "    y = df['PlayTennis']\n",
    "\n",
    "    # Build the tree\n",
    "    tree = c50(X, y, X.columns, max_depth=3)\n",
    "    print(\"Decision Tree:\", tree)\n",
    "\n",
    "    # Predict a sample\n",
    "    sample = {'Outlook': 'Sunny', 'Temperature': 'Cool', 'Humidity': 'High', 'Wind': 'Strong'}\n",
    "    prediction = predict(tree, sample)\n",
    "    print(\"Prediction for sample:\", prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification and Regression Trees Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Function to calculate Gini impurity for classification\n",
    "def gini_impurity(y):\n",
    "    counts = Counter(y)\n",
    "    probabilities = [count / len(y) for count in counts.values()]\n",
    "    return 1 - sum(p ** 2 for p in probabilities)\n",
    "\n",
    "# Function to calculate Mean Squared Error for regression\n",
    "def mean_squared_error(y):\n",
    "    mean = np.mean(y)\n",
    "    return np.mean((y - mean) ** 2)\n",
    "\n",
    "# Function to calculate the impurity for a given split\n",
    "def calculate_impurity(X, y, split_attribute, split_value, task='classification'):\n",
    "    left_mask = X[split_attribute] <= split_value\n",
    "    right_mask = X[split_attribute] > split_value\n",
    "    \n",
    "    if task == 'classification':\n",
    "        left_impurity = gini_impurity(y[left_mask])\n",
    "        right_impurity = gini_impurity(y[right_mask])\n",
    "    elif task == 'regression':\n",
    "        left_impurity = mean_squared_error(y[left_mask])\n",
    "        right_impurity = mean_squared_error(y[right_mask])\n",
    "    \n",
    "    left_weight = len(y[left_mask]) / len(y)\n",
    "    right_weight = len(y[right_mask]) / len(y)\n",
    "    \n",
    "    return left_weight * left_impurity + right_weight * right_impurity\n",
    "\n",
    "# Function to find the best split for the dataset\n",
    "def find_best_split(X, y, attributes, task='classification'):\n",
    "    best_impurity = float('inf')\n",
    "    best_split = None\n",
    "    \n",
    "    for attribute in attributes:\n",
    "        values = np.unique(X[attribute])\n",
    "        for value in values:\n",
    "            impurity = calculate_impurity(X, y, attribute, value, task)\n",
    "            if impurity < best_impurity:\n",
    "                best_impurity = impurity\n",
    "                best_split = (attribute, value)\n",
    "    \n",
    "    return best_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, task='classification', max_depth=None, min_samples_split=2):\n",
    "        self.task = task\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "    \n",
    "    def _build_tree(self, X, y, depth):\n",
    "        if len(set(y)) == 1:\n",
    "            return y.iloc[0] if self.task == 'classification' else np.mean(y)\n",
    "        if len(y) < self.min_samples_split or (self.max_depth is not None and depth >= self.max_depth):\n",
    "            return Counter(y).most_common(1)[0][0] if self.task == 'classification' else np.mean(y)\n",
    "        \n",
    "        best_split = find_best_split(X, y, X.columns, self.task)\n",
    "        if best_split is None:\n",
    "            return Counter(y).most_common(1)[0][0] if self.task == 'classification' else np.mean(y)\n",
    "        \n",
    "        attribute, value = best_split\n",
    "        tree = {attribute: {}}\n",
    "        \n",
    "        left_mask = X[attribute] <= value\n",
    "        right_mask = X[attribute] > value\n",
    "        \n",
    "        left_subtree = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "        \n",
    "        tree[attribute]['<= {}'.format(value)] = left_subtree\n",
    "        tree[attribute]['> {}'.format(value)] = right_subtree\n",
    "        \n",
    "        return tree\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X.apply(self._predict_sample, axis=1, args=(self.tree,))\n",
    "    \n",
    "    def _predict_sample(self, sample, tree):\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "        attribute = next(iter(tree))\n",
    "        value = float(next(iter(tree[attribute].keys())).split()[1])\n",
    "        if sample[attribute] <= value:\n",
    "            return self._predict_sample(sample, tree[attribute]['<= {}'.format(value)])\n",
    "        else:\n",
    "            return self._predict_sample(sample, tree[attribute]['> {}'.format(value)])\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example dataset for classification\n",
    "    data = {\n",
    "        'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],\n",
    "        'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],\n",
    "        'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],\n",
    "        'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],\n",
    "        'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\n",
    "    }\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Features and labels for classification\n",
    "    X_classification = df.drop(columns='PlayTennis')\n",
    "    y_classification = df['PlayTennis']\n",
    "\n",
    "    # Build and fit the classification tree\n",
    "    tree_classification = DecisionTree(task='classification', max_depth=3)\n",
    "    tree_classification.fit(X_classification, y_classification)\n",
    "    print(\"Classification Tree:\", tree_classification.tree)\n",
    "\n",
    "    # Predict a sample for classification\n",
    "    sample_classification = {'Outlook': 'Sunny', 'Temperature': 'Cool', 'Humidity': 'High', 'Wind': 'Strong'}\n",
    "    prediction_classification = tree_classification.predict(pd.DataFrame([sample_classification]))\n",
    "    print(\"Prediction for classification sample:\", prediction_classification.iloc[0])\n",
    "\n",
    "    # Example dataset for regression\n",
    "    data_regression = {\n",
    "        'Feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'Feature2': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
    "        'Target': [2.3, 2.1, 3.6, 3.8, 5.0, 5.2, 7.8, 7.6, 8.5, 8.7]\n",
    "    }\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_regression = pd.DataFrame(data_regression)\n",
    "\n",
    "    # Features and labels for regression\n",
    "    X_regression = df_regression.drop(columns='Target')\n",
    "    y_regression = df_regression['Target']\n",
    "\n",
    "    # Build and fit the regression tree\n",
    "    tree_regression = DecisionTree(task='regression', max_depth=3)\n",
    "    tree_regression.fit(X_regression, y_regression)\n",
    "    print(\"Regression Tree:\", tree_regression.tree)\n",
    "\n",
    "    # Predict a sample for regression\n",
    "    sample_regression = {'Feature1': 5, 'Feature2': 6}\n",
    "    prediction_regression = tree_regression.predict(pd.DataFrame([sample_regression]))\n",
    "    print(\"Prediction for regression sample:\", prediction_regression.iloc[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GitHub-Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
